---
title: "binomial_baseline"
author: 'Jongwoo Choi'
date: "`r format(Sys.Date())`"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(knitr);library(arm);library(ggplot2)
library(MASS);library(tidyr);library(dplyr)
library(extraDistr);library(gridExtra)
library(rstan);library(bayesplot);library(loo)
library(shinystan);library(readr)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, error=FALSE, comment=NA)
options(xtable.comment = FALSE)
options(htmltools.dir.version = FALSE)
options(digits = 2)

print_file <- function(file) {
  cat(paste(readLines(file), "\n", sep=""), sep="")
}
```

```{r Load the data, message=FALSE, warning=FALSE}
file <-  'core.txt'
data <- read_delim(file = file, delim = '|')

# Sample the data
pct = 1
# pct = 0.1
# pct = 0.01
set.seed(seed = 42)
sample_size = round(pct * nrow(data))
sample <- sample(x = nrow(data), size = sample_size, replace = F)
data = data[sample, ]
## Selecting the relevant columns for the analysis
data_sub <- data %>% dplyr::select(
  state,
  city,
  county,
  zip,
  asset_market_value,
  mar_2_app,
  appraisal_value,
  app_2_inc,
  client_income,
  mar_2_inc,
  age,
  sex_F,
  condition_U,
  y,
  y2)
summary(data_sub)


## Group data by state and define the IDs
state_summary <- data_sub %>% 
  dplyr::select(state, 
                client_income, 
                appraisal_value,
                asset_market_value) %>% 
  group_by(state) %>% 
  summarize(n_state = n(),
            income_mean_state = mean(client_income),
            appraisal_mean_state = mean(appraisal_value),
            market_mean_state = mean(asset_market_value)) %>% 
  arrange(desc(n_state)) %>% 
  ungroup()
state_summary$ID_state = seq.int(nrow(state_summary))


## Group data by city and define the IDs
city_summary <- data_sub %>% 
  dplyr::select(city, state,
                client_income,
                appraisal_value,
                asset_market_value,
                mar_2_inc,
                mar_2_app,
                app_2_inc,
                age,
                y,
                y2) %>% 
  group_by(city, state) %>% 
  summarize(n_city = n(),
            income_mean_city = mean(client_income),
            appraisal_mean_city = mean(appraisal_value),
            market_mean_city = mean(asset_market_value),
            mar_2_inc_mean_city = mean(mar_2_inc),
            mar_2_app_mean_city = mean(mar_2_app),
            app_2_inc_mean_city = mean(app_2_inc),
            age_mean_city = mean(age),
            sum_y = sum(y),
            sum_y2 = sum(y2)) %>% 
  arrange(desc(n_city)) %>% 
  ungroup()


## Merge back into data
city_summary <- city_summary %>% 
  inner_join(y = state_summary[c('ID_state', 'state')], by = 'state')


## Rescaling
inputs <- city_summary %>%
  mutate(
    market_state_city = (log(market_mean_city) - mean(log(market_mean_city))) /
      sd(log(market_mean_city)),
    
    income_state_city = (log(appraisal_mean_city) - mean(log(appraisal_mean_city))) /
      sd(log(appraisal_mean_city)),
    
    appraisal_state_city = (log(appraisal_mean_city) -
                            mean(log(appraisal_mean_city))) /
      sd(log(appraisal_mean_city)),
    
    mar_2_inc_city = (mar_2_inc_mean_city - mean(mar_2_inc_mean_city)) / sd(mar_2_inc_mean_city),
    
    app_2_inc_city = (app_2_inc_mean_city - mean(app_2_inc_mean_city)) / sd(app_2_inc_mean_city),
    
    mar_2_app_city = (mar_2_app_mean_city - mean(mar_2_app_mean_city)) / sd(mar_2_app_mean_city),
    
    age_city = (age_mean_city - mean(age_mean_city)) / sd(age_mean_city)) %>% 
  dplyr::select(
    market_state_city,
    income_state_city,
    appraisal_state_city,
    mar_2_inc_city,
    app_2_inc_city,
    mar_2_app_city,
    age_city,
    ID_state,
    n_city,
    sum_y,
    sum_y2
  )
```


## Baseline Model: Binomial 

Our baseline model is simple binomial regression model. We give weak cauchy priors on the coefficient parameter $\beta$ the intercept $a$.
In the city level, we assume that the number of individual records in city $i$ is $n_i$.
$$
a \sim \mathsf{Cauchy}(0, \; 10)
$$
$$
\beta \sim \mathsf{Cauchy}(0, \; 2.5)
$$

$$
y_{i} \sim \mathsf{Binomial} (n_{i}, \; logit^{-1}(a + \beta \cdot X_{i}))
$$

The binomial baseline model is given by:
```{r results='hide', warning=FALSE, message=FALSE, cache=TRUE}
baseline_model_binom=stan_model('binomial_baseline_city.stan')
```

```{r}
print_file('binomial_baseline_city.stan')
```


### Parameters recovered

We generate the fake data to simulate the model. 
```{r}
a <- rcauchy(1, 0, 10)
beta <- rcauchy(7, 0, 2.5)
X <- inputs %>% dplyr::select(-ID_state,-n_city,-sum_y, -sum_y2)
n_city <- inputs$n_city
N = nrow(X)
D = ncol(X)

y_fake <- c()
for (i in 1:N){
  y_fake[i] <- rbinom(1, n_city[i], 
                      invlogit(a + beta %*% as.matrix(X)[i,]))
}

fake_baseline_data <- list(N_train=N, N_test=N, D=D, 
                           X_train=X, X_test=X,
                           n_city_train = n_city, 
                           n_city_test = n_city,
                           y_train = y_fake)
```


```{r results='hide', warning=FALSE, message=FALSE, cache=TRUE, eval=FALSE}
fit_fake <- sampling(baseline_model_binom, 
                     data=fake_baseline_data, seed=1234)
saveRDS(fit_fake, file = 'baseline_fit_fake.rds')
```

```{r}
fit_fake <- readRDS(file = 'baseline_fit_fake.rds')
print(fit_fake, pars = c('a', 'beta'),
      digits = 2, probs = c(0.025, 0.5, 0.975))
```


```{r}
plot(fit_fake)
```


The following plot shows how well parameters recovered. 
```{r}
sims_fake <- as.matrix(fit_fake)
true <- c(a, beta)
color_scheme_set("brightblue")
mcmc_recover_hist(sims_fake[, 1:8], true)
```

We see that most $\beta$'s didn't recovered well except $beta_{6}$ and $\beta_{7}$.


### Fit real data

Now we fit the real data with this model.

```{r}
y = inputs$sum_y
#y2 = inputs$sum_y2

baseline_data = list(N_train=N, N_test=N, D=D, 
                     X_train=X, X_test=X,
                     n_city_train = n_city, 
                     n_city_test = n_city,
                     y_train = y)
```

```{r results='hide', warning=FALSE, message=FALSE, cache=TRUE, eval=FALSE}
fit1 <- sampling(baseline_model_binom,
                 data=baseline_data, seed=1234)
saveRDS(fit1, file = 'baseline_fit1.rds')
```


The following shows the posterior mean and sd of our parameters.
```{r}
fit1 <- readRDS(file = 'baseline_fit1.rds')
print(fit1, pars=c('a', 'beta', 'lp__'),
      digits = 2, probs = c(0.025, 0.5, 0.975))
```

The posterior mean of intercept $a$ is $-2.68$ with standard deviation $0.04$. We can also see the posterior mean and standard deviation of coefficient parameter $\beta$ from the above table. We see that $\beta_{2}$ and $\beta_{3}$
have larger standard deviation compare to others. 



### PPC

In stan model, we generated $y_rep$ of city level using `binomial_rng` function in stan.

```{r}
'transformed parameters {
  vector[N_train] eta;
  eta = a + X_train*beta;               // probability in binomial regression
}

generated quantities{ 
  int<lower =0> y_rep[N_train];
  for (i in 1:N_train){
    y_rep[i] = binomial_rng(n_city_train[i], inv_logit(eta[i]));
  }
}'
```


The following shows the posterior predictive check. 

```{r}
y_rep <- as.matrix(fit1, pars = "y_rep")
ppc_dens_overlay(y = y, y_rep[1:200,])
```

We see that the new $y$ fit well even though there were some gaps. 


```{r}
sims <- rstan::extract(fit1)

df <- data.frame(y_rep_mean = apply(X=sims$y_rep, MARGIN = 1, FUN = mean))
meangg <- ggplot(df, aes(x=y_rep_mean)) +
  geom_histogram(fill='lightblue', color='black') +
  geom_vline(xintercept = mean(y), color='red') +
  ggtitle('mean of y_rep')

df <- data.frame(y_rep_sd = apply(X = sims$y_rep, MARGIN = 1, FUN = sd))
sdgg <- ggplot(df, aes(x=y_rep_sd)) +
  geom_histogram(fill='lightblue', color='black') +
  geom_vline(xintercept = sd(y), color='red') +
  ggtitle('sd of y_rep')

df <- data.frame(y_rep_max = apply(X = sims$y_rep, MARGIN = 1, FUN = max))
maxgg <- ggplot(df, aes(x=y_rep_max)) +
  geom_histogram(fill='lightblue',color='black') +
  geom_vline(xintercept = max(y), color='red') +
  ggtitle('max of y_rep')

df <- data.frame(y_rep_min = apply(X = sims$y_rep, MARGIN = 1, FUN = min))
mingg <- ggplot(df, aes(x=y_rep_min)) +
  geom_histogram(fill='lightblue',color='black') +
  geom_vline(xintercept = min(y), color='red') +
  ggtitle('min of y_rep')

gridExtra::grid.arrange(meangg, sdgg, maxgg, mingg,
                        layout_matrix = rbind(c(1, 2), 
                                              c(3, 4)))

```


```{r}
# Scatterplot of two test statistics
ppc_scatter_avg(y = y, yrep = y_rep)
```

Scattor plot looks linear but it is not perfect. 


```{r echo=FALSE, include=FALSE, eval=FALSE}
library('shinystan')
shiny_base = as.shinystan(fit1)
#launch_shinystan(shiny_base)
```


### Evaluation (RMSE)

We evaluate our training model with RMSE using 5 fold cross validation.

```{r results='hide', warning=FALSE, message=FALSE, cache=TRUE}
## K fold CV
set.seed(1234)
splited_inputs <- split(inputs, sample(rep(1:5, 176)))

for (i in 1:5){
  a <- c(1,2,3,4,5)[-i]
  inputs_test = splited_inputs[[i]]
  inputs_train = rbind(splited_inputs[[a[1]]],
                  splited_inputs[[a[2]]],
                  splited_inputs[[a[3]]],
                  splited_inputs[[a[4]]]) 
  
  y_train = inputs_train$sum_y
  y_test = inputs_test$sum_y
  
  ## Inputs for STAN
  n_city_train = inputs_train$n_city
  n_city_test = inputs_test$n_city
  
  
  X_train = inputs_train %>% 
    dplyr::select(-ID_state,-n_city,-sum_y, -sum_y2)
  X_test = inputs_test %>% 
    dplyr::select(-ID_state,-n_city,-sum_y, -sum_y2)
  
  N_train = nrow(X_train)
  N_test = nrow(X_test)
  
  D = ncol(X_train)

  baseline_data = list(N_train=N_train, N_test=N_test, D=D,
                       X_train=X_train, X_test=X_test,
                       n_city_train = n_city_train, 
                       n_city_test = n_city_test,
                       y_train = y_train)
  fit_cv <- sampling(baseline_model_binom, data=baseline_data, seed=1234)
  name = paste('model1_cv', as.character(i), '.rds', sep = "")
  saveRDS(fit_cv, file = name)
}

```

```{r}
rmse <- c()
for (i in 1:5){
  name = paste('model1_cv', as.character(i),'.rds',sep = "")
  fit_cv <- readRDS(file = name)
  sims_cv <- rstan::extract(fit_cv)
  y_hat <- apply(X = sims_cv$y_rep_cv, MARGIN = 2, FUN = median)  
  
  test_df = data.frame(ID_state = inputs_test$ID_state,
                       y_test = y_test,
                       y_hat = y_hat)
  
  test_df <- test_df %>% 
    summarize(y_sum_test = sum(y_test),
              y_sum_hat = sum(y_hat)) %>% 
    arrange(desc(y_sum_test)) %>% 
    ungroup()
  
  #mse_baseline = mean((test_df$y_sum_test) ** 2)
  rmse[i] = sqrt(mean((test_df$y_sum_hat - test_df$y_sum_test) ** 2))
}

average_RMSE <- mean(rmse)
sd_RMSE <- sd(rmse)

cat('The average of RMSE of baseline model is: ', average_RMSE)
cat('The standard deviation of RMSE of baseline model is: ', sd_RMSE)

```







