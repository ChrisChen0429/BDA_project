---
title: "binomial_baseline"
author: 'Jongwoo Choi'
date: "`r format(Sys.Date())`"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
library(knitr);library(arm);library(ggplot2)
library(MASS);library(tidyr);library(dplyr)
library(extraDistr);library(gridExtra)
library(rstan);library(bayesplot);library(loo)
library(shinystan);library(readr)
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE, error=FALSE, comment=NA)
options(xtable.comment = FALSE)
options(htmltools.dir.version = FALSE)
options(digits = 2)

print_file <- function(file) {
  cat(paste(readLines(file), "\n", sep=""), sep="")
}
```

```{r Load the data, message=FALSE, warning=FALSE}
file <-  'core.txt'
data <- read_delim(file = file, delim = '|')

# Sample the data
pct = 1
# pct = 0.1
# pct = 0.01
set.seed(seed = 42)
sample_size = round(pct * nrow(data))
sample <- sample(x = nrow(data), size = sample_size, replace = F)
data = data[sample, ]
## Selecting the relevant columns for the analysis
data_sub <- data %>% dplyr::select(
  state,
  city,
  county,
  zip,
  asset_market_value,
  mar_2_app,
  appraisal_value,
  app_2_inc,
  client_income,
  mar_2_inc,
  age,
  sex_F,
  condition_U,
  y,
  y2)
summary(data_sub)


## Group data by state and define the IDs
state_summary <- data_sub %>% 
  dplyr::select(state, 
                client_income, 
                appraisal_value,
                asset_market_value) %>% 
  group_by(state) %>% 
  summarize(n_state = n(),
            income_mean_state = mean(client_income),
            appraisal_mean_state = mean(appraisal_value),
            market_mean_state = mean(asset_market_value)) %>% 
  arrange(desc(n_state)) %>% 
  ungroup()
state_summary$ID_state = seq.int(nrow(state_summary))


## Group data by city and define the IDs
city_summary <- data_sub %>% 
  dplyr::select(city, state,
                client_income,
                appraisal_value,
                asset_market_value,
                mar_2_inc,
                mar_2_app,
                app_2_inc,
                age,
                y,
                y2) %>% 
  group_by(city, state) %>% 
  summarize(n_city = n(),
            income_mean_city = mean(client_income),
            appraisal_mean_city = mean(appraisal_value),
            market_mean_city = mean(asset_market_value),
            mar_2_inc_mean_city = mean(mar_2_inc),
            mar_2_app_mean_city = mean(mar_2_app),
            app_2_inc_mean_city = mean(app_2_inc),
            age_mean_city = mean(age),
            sum_y = sum(y),
            sum_y2 = sum(y2)) %>% 
  arrange(desc(n_city)) %>% 
  ungroup()


## Merge back into data
city_summary <- city_summary %>% 
  inner_join(y = state_summary[c('ID_state', 'state')], by = 'state')


## Rescaling
inputs <- city_summary %>%
  mutate(
    market_state_city = (log(market_mean_city) - mean(log(market_mean_city))) /
      sd(log(market_mean_city)),
    
    income_state_city = (log(appraisal_mean_city) - mean(log(appraisal_mean_city))) /
      sd(log(appraisal_mean_city)),
    
    appraisal_state_city = (log(appraisal_mean_city) -
                            mean(log(appraisal_mean_city))) /
      sd(log(appraisal_mean_city)),
    
    mar_2_inc_city = (mar_2_inc_mean_city - mean(mar_2_inc_mean_city)) / sd(mar_2_inc_mean_city),
    
    app_2_inc_city = (app_2_inc_mean_city - mean(app_2_inc_mean_city)) / sd(app_2_inc_mean_city),
    
    mar_2_app_city = (mar_2_app_mean_city - mean(mar_2_app_mean_city)) / sd(mar_2_app_mean_city),
    
    age_city = (age_mean_city - mean(age_mean_city)) / sd(age_mean_city)) %>% 
  dplyr::select(
    market_state_city,
    income_state_city,
    appraisal_state_city,
    mar_2_inc_city,
    app_2_inc_city,
    mar_2_app_city,
    age_city,
    ID_state,
    n_city,
    sum_y,
    sum_y2
  )


## Train / Test split
set.seed(seed = 1234)
pct_train = 0.8
sample_size = round(pct_train * nrow(inputs))
sample <- sample(x = nrow(inputs), size = sample_size, replace = F)

## Allocate train
y = inputs$sum_y
y2 = inputs$sum_y2

inputs_train = inputs[sample, ]
y_train = y[sample]

## Allocate test
inputs_test = inputs[-sample, ]
y_test = y[-sample]


## Inputs for STAN
X_train = inputs_train %>% dplyr::select(-ID_state,-n_city,-sum_y, -sum_y2)
X_test = inputs_test %>% dplyr::select(-ID_state,-n_city,-sum_y, -sum_y2)

N_train = nrow(X_train)
N_test = nrow(X_test)

n_city_train = inputs_train$n_city
n_city_test = inputs_test$n_city

D = ncol(X_train)

baseline_data = list(N_train=N_train, N_test=N_test, D=D, 
                     X_train=X_train, X_test=X_test,
                     n_city_train = n_city_train, 
                     n_city_test = n_city_test,
                     y_train = y_train)
```


## Baseline Model: Binomial 

Our baseline model is binomial. We give cauchy priors on the coefficient parameter $\beta$ the intercept $a$.
In the city level, we assume that the number of individual records in city $c$ is $n_c$.
$$
a \sim \mathsf{Cauchy}(0, 10)
$$
$$
\beta \sim \mathsf{Cauchy}(0, 2.5)
$$

$$
y_{c} \sim \mathsf{Binomial} (N_{c}, \; logit^{-1}(a + \beta \cdot X_{c}))
$$

The binomial baseline model is given by:
```{r}
print_file('binomial_baseline_city.stan')
```

### Parameters recovered

We generate the fake data to simulate the model. 
```{r}
a <- rcauchy(1, 0, 10)
beta <- rcauchy(7, 0, 2.5)

y_fake <- c()
for (i in 1:N_train){
  y_fake[i] <- rbinom(1, n_city_train[i], 
                      invlogit(a + beta %*% as.matrix(X_train)[i,]))
}

fake_baseline_data <- list(N_train=N_train, N_test=N_test, D=D, 
                           X_train=X_train, X_test=X_test,
                           n_city_train = n_city_train, 
                           n_city_test = n_city_test,
                           y_train = y_fake)
```

```{r results='hide', warning=FALSE, message=FALSE, cache=TRUE}
baseline_model_binom=stan_model('binomial_baseline_city.stan')
fit_fake <- sampling(baseline_model_binom, data=fake_baseline_data, seed=1234)
```

```{r}
print(fit_fake, pars = c('a', 'beta'),
      digits = 2, probs = c(0.025, 0.5, 0.975))
```

```{r}
sims_fake <- as.matrix(fit_fake)
true <- c(a, beta)
color_scheme_set("brightblue")
mcmc_recover_hist(sims_fake[, 1:8], true)
```


### Fit real data
Now we fit the real data with this model.
```{r results='hide', warning=FALSE, message=FALSE, cache=TRUE}
#baseline_model_binom=stan_model('binomial_baseline_city.stan')
fit1 <- sampling(baseline_model_binom, data=baseline_data, seed=1234)
```

```{r}
print(fit1, pars=c('a', 'beta', 'lp__'),
      digits = 2, probs = c(0.025, 0.5, 0.975))
```


### PPC

```{r}
sims <- rstan::extract(fit1)
y_max <- apply(X = sims$y_rep, MARGIN = 1, FUN = max)

df <- data.frame(y_rep_max = y_max)
ggplot(df, aes(x=y_rep_max)) +
  geom_histogram(fill='lightblue',
                 color='black') +
  geom_vline(xintercept = max(y), color='red') +
  ggtitle('max(y_rep)')
```

```{r STD, message=FALSE, warning=FALSE}
df <- data.frame(y_rep_sd = apply(X = sims$y_rep, MARGIN = 1, FUN = sd))
ggplot(df, aes(x=y_rep_sd)) +
  geom_histogram(fill='lightblue',
                 color='black') +
  geom_vline(xintercept = sd(y), color='red') +
  ggtitle('sd(y_rep)')
```

```{r}
y_rep <- as.matrix(fit1, pars = "y_rep")
ppc_dens_overlay(y = y_train, y_rep[1:200,])
```

```{r echo=FALSE, include=FALSE}
library('shinystan')
shiny_base = as.shinystan(fit1)
#launch_shinystan(shiny_base)
```

### MSE
```{r}
# a_median <- median(sims$a)
# beta_median <- apply(X = sims$beta, MARGIN = 2, FUN = median)
# 
# y_hat <- N_test * invlogit(a_median + as.matrix(X_test)%*%beta_median)
# mode <- function(x) {
#   ux <- unique(x)
#   ux[which.max(tabulate(match(x, ux)))]
# }

y_hat <- apply(X = sims$y_rep_cv, MARGIN = 2, FUN = median)  

test_df = data.frame(ID_state = inputs_test$ID_state,
                     y_test = y_test,
                     y_hat = y_hat)

test_df <- test_df %>% 
  summarize(y_sum_test = sum(y_test),
            y_sum_hat = sum(y_hat)) %>% 
  arrange(desc(y_sum_test)) %>% 
  ungroup()

accuracy_baseline = mean(abs(test_df$y_sum_test) ** 2)
accuracy = mean(abs(test_df$y_sum_hat - test_df$y_sum_test) ** 2)
cat('\nBaseline MSE: ', accuracy_baseline * 100)
cat('\nLogistic MSE: ', accuracy * 100)
```







