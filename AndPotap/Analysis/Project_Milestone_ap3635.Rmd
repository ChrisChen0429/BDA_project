---
title: "FoGM: Project Milestone"
author: "Andres Potapczynski (ap3635)"
output: pdf_document
---

```{r, include=FALSE}
## Don't forget to fill in the missing values for [...]
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r Imports, include=FALSE}
library(rstan)
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(bayesplot)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

```{r Files, include=FALSE}
file <-  '../DBs/core.txt'
file_mode_betabin <- './zip_code_v01.stan'
```

The current document is ordered in the following manner. First, I set the context of the problem by describing the data set and some key features about it. Here I also re-state the questions that we (start-up and I) want to answer. Next I show some preliminary models that I ran in order to make sense of the data. The idea is to find the set variables that we should include in the final model and to understand at what level of granularity are those variables meaningful. Then I show a proposal of a graphical model that I then plan to estimate via VI. Finally, I conclude with a pipeline of models that I will be running in the following month as well as some questions where I would like to get some feedback on.

# Setting the preamble

```{r Load the data, include=FALSE}
data <- read_delim(file = file, delim = '|')

# Sample the data
# pct = 1
# pct = 0.1
pct = 0.01
set.seed(seed = 42)
sample_size = round(pct * nrow(data))
sample <- sample(x = nrow(data), size = sample_size, replace = F)
data = data[sample, ]

## Filtering the correct data set
print('The current dimension of the data is')
print(dim(data_sub))

## Selecting the relevant columns for the analysis
data_sub <- data %>% select(client_income,
                        ratio,
                        age,
                        sex_F,
                        condition_U,
                        effective_pay,
                        factor_employed,
                        inv,
                        risk_index,
                        y)
summary(data_sub)

## Filter incorrect values
inputs <- data_sub %>% 
  filter(risk_index > 0,
         effective_pay > 0)
print('Filtering makes us')
print(dim(inputs))
y = inputs$y
inputs <- inputs %>% select(-y)
summary(inputs)

## Rescaling
inputs <- inputs %>% 
  mutate_at(.funs = scale, .vars = c('client_income',
                                     'ratio',
                                     'age',
                                     'effective_pay',
                                     'factor_employed',
                                     'risk_index'))


## Creating the data set for the zip code analysis
data_sub <- data %>% 
  group_by(state, city, zip) %>% 
  summarize(cases = sum(y),
            counts = n(),
            theta_emp = cases / counts) %>% 
  arrange(desc(cases)) %>% 
  ungroup()
summary(data_sub)

## Shuffle
sample_entries <- sample(x = nrow(data_sub), size = nrow(data_sub), replace = F)
data_sub = data_sub[sample_entries, ]

## Create inputs for STAN
y_beta_bin = data_sub$cases
N_z = data_sub$counts
inputs_stan = list(M=nrow(data_sub), N=N_z, y=y_beta_bin)
```

The data set was provided by a mortgage lending start-up company in Mexico. This data set consists of __30,499 mortgages with over 90 covariates__ (where some of features are simply administrative and thus were not included in the analysis). __The average default rate is 6%__. Also, the data set provided was __their latest report available at August 2018__; if needed, I could incorporate historical information of previous months. Some of the key variables are:

* __[Location]__ `state`, `city`and `zip`: These features will allow me to understand if the client's behavior varies by geography. Also, I have this location features for both the house acquired and from owner's location.

* __[Demographics]__ `age`, `sex`, `ratio`, `risk_index`, `client_income` and `credit_score`. The feature `ratio` is the % of the client's income that the monthly payments for its mortgage represent. The `risk_index` is a variable that combines several metrics like [...] to evaluate the risk inherent in that person. The `client_income` feature is the amount of money that the person earned at that moment in time when she signed the mortgage. Finally, the ordinal variable `credit_score` evaluates how the client has performed in previous debts.

* __[Asset features]__ `vendor name`, `new_used`, `inv` and `appraisal_value`: These features tell me who was the vendor (either a construction company or an individual), if the asset is new or not, then `inv` (made up feature) is a dummy variable that takes the value of 1 if the house bought is in the same state where the owner lives and the last variable is the price of the asset.

* __[Employment]__ `employer_name` and `factor_employed`. The first feature conveys who is the employer and the second one information about the client's status of employment.

* __[Payment Records]__ `days_pay`, and `y`. The first feature counts the number of days that have passed between the last payment date and the date on which the mortgage started. The next feature is the my target variable that is 1 if the mortgage has at least one month of no payment and 0 otherwise. 

Note that features like `interest_rate` and `contract_length` are present but have no variance. The current _product offer_ in the data base is a 12% interest rate mortgage for 30 years. Thus it is impossible to pose counter factual questions for those variables. 

The empirical distribution of some of the previous key variables is

```{r Display all, echo=FALSE}
market_value <- data %>% filter(asset_market_value < 1600000)
ratio_hist <- data %>% filter(ratio > 0.20)
risk_hist <- data %>% filter(risk_index > 1900, risk_index < 2400)
zip_hist <- data %>% group_by(zip, state, city) %>% 
  summarize(mean_income = mean(client_income)) %>% 
  arrange(desc(mean_income)) %>% 
  ungroup()

## Market value distribution
g1 <- ggplot(data = market_value, mapping = aes(x = asset_market_value / 1000)) +
  geom_histogram(fill='lightblue', color='black', bins = 30) +
  labs(title = 'Houses concentrated at $300-600',
       subtitle = 'Asset Market value distribution') +
  ylab(NULL) +
  xlab('K MXN') +
  theme(plot.title = element_text(size = 10, face = "bold")) +
  theme(plot.subtitle = element_text(size = 8)) +
  theme(axis.title.x = element_text(size = 8))

## Ratio distribution
g2 <- ggplot(data = ratio_hist, mapping = aes(x = ratio)) +
  geom_histogram(fill='lightblue', color='black', bins = 30) +
  labs(title = 'No over 30% mortgages',
       subtitle = 'Ratio (Mortgage / Income) distribution') +
  ylab(NULL) +
  xlab('ratio') +
  theme(plot.title = element_text(size = 10, face = "bold")) +
  theme(plot.subtitle = element_text(size = 8)) +
  theme(axis.title.x = element_text(size = 8))

## Risk index distribution
g3 <- ggplot(data = risk_hist, mapping = aes(x = risk_index)) +
  geom_histogram(fill='lightblue', color='black', bins = 30) +
  labs(title = 'Similar risks measurements',
       subtitle = 'Risk index distribution') +
  ylab(NULL) +
  xlab('risk index (points)') +
  theme(plot.title = element_text(size = 10, face = "bold")) +
  theme(plot.subtitle = element_text(size = 8)) +
  theme(axis.title.x = element_text(size = 8))

## Zipcode income distribuion
g4 <- ggplot(data = zip_hist, mapping = aes(x = mean_income)) +
  geom_histogram(fill='lightblue', color='black', bins = 30) +
  labs(title = 'Severly skewed',
       subtitle = 'Income per zipcode distribution') +
  ylab(NULL) +
  xlab('MXN') +
  theme(plot.title = element_text(size = 10, face = "bold")) +
  theme(plot.subtitle = element_text(size = 8)) +
  theme(axis.title.x = element_text(size = 8))

## Diplay all the graphs
gridExtra::grid.arrange(g1, g4, g2, g3,
                        layout_matrix = rbind(c(1, 2), 
                                              c(3, 4)))
```

\newpage

# Understanding the data

Personally, the best way to understand a data set is to run different models and see where they fail. I ran the next set of models in `STAN`. `STAN` is good right now for this aggregate top-down analysis, however, for my final model the complexity will require me to derive VI updates.

__Model 1 (Simple Logistic Regression)__:

$$
\beta \sim N(\mu, \sigma ^ 2)
$$
$$
y_i \sim Ber(logistic^{-1}(X \beta))
$$
where $i=1, \dots ,N$ where $N$ is the total number of mortgages. Also, $X$ is augmented to include an intercept. Below are the covariates used with their MAP estimates. They were all standardize before plugging them as inputs.

```{r Run first STAN model, include=FALSE}
inputs_stan = list(N=nrow(inputs), D=ncol(inputs), X=inputs, y=y)
sm <- stan_model('./logistic_reg_v01.stan')
sm.logistic1 = sampling(sm, data=inputs_stan, iter=1000, chains=4)
```
```{r Print resutls of logistic regression, echo=FALSE}
print(sm.logistic1, digits = 2)
```


Variable | MAP Estimate
------------- | -------------
$\beta_0$ | -3.47
`client_income` | -0.37
`ratio` | -0.37
`age` | 0.06
`sex` | 0.14
`new_used` | -0.01
`days_pay` | -1.37
`factor_employed` | -0.22
`inv` | 0.05
`risk_index` | -0.21
`lp` | -

at this aggregate level the only relevant variable appears to amounts of days that the client has payed his mortgage `days_pay` and $\beta_0$.

__Model 2 (Location Hierarchical Logistic Regression)__:

$$
\alpha \sim Ga(a_0, b_0)
$$
$$
\beta \sim Ga(a_0, b_0)
$$
where for each zip code
$$
\theta_j \sim Beta(\alpha, \beta)
$$
$$
y_j \sim Bin(n_j, \theta_j)
$$
where now $y_j$ represents the accumulated cases of default per zip code. The idea here it to uncover if there are some areas that are more prone to default.

```{r Generate data for the first approach, include=FALSE}
compile_betabin <- stan_model(file_mode_betabin)
sm.betabin = sampling(compile_betabin, data=inputs_stan)
```

```{r Print results of beta-binomial, echo=FALSE}
print(sm.betabin, digits=2, pars = c('alpha', 'beta', 'lp__'))
```

```{r Extract sims betabin, echo=FALSE}
sims.betabin = rstan::extract(sm.betabin)
theta.betabin = apply(X = sims.betabin$theta, MARGIN = 2, FUN = median)
data_sub$theta.betabin = theta.betabin
ggplot(data = data_sub, mapping = aes(x = theta.betabin, y = theta_emp)) +
  geom_point(color = 'grey') +
  geom_point(aes(x = theta.betabin, y = theta_emp), color='blue') +
  ylim(0, 1) +
  labs(title = 'Information sharing achieves more reasonable estimates for each zip code',
       subtitle = 'Hierarchical Logistic Regression estimates of default rates per zip code') +
  ylab('Observed default rate') +
  xlab('Estimated default rate') +
  theme(plot.title = element_text(size = 12, face = "bold")) +
  theme(plot.subtitle = element_text(size = 10)) +
  theme(axis.title.x = element_text(size = 10))
```

## Questions we want to answer with the data (if possible)

Below I restate the questions that we want to answer (in order of importance):

* __Why do people default?__: Generally put, this is the question that will enable us to answer all the questions below but, eventhough it is easy to understand, it is hard to pin down formally. Next are the our main hypothesis of why people default. One hypothesis is the customer might have run into some financial difficulties. Another hypothesis is that the customer might not like the property that she bought and thus abandon it. Yet another hypothesis is that some conditions in the area where the customer lives have made him move away and thus abandon the property. One final hypothesis is that the customer has bought this asset as an investment but it is not interested in keeping it anymore. The answer might be a mixture of all these plausible stories, yet we want to uncover which is more consistent with the data and an appropriate model formulation to address each.

* __Why are some geographical areas more prone to default?__: Since 2010 Mexico started a drug war against different cartels spread throughout the country. Still today in 2018, there are some states that are still in conflict. We want to understand what is the collateral damage that this event has on the ability of people adquiring mortgages in those areas. Also, as seen in the previous analysis, we have uncover that some states where there is a drug conflict should a higher probability of defaulting.

* __Are there some unstrustful vendors?__: We have been told that one of the main reasons that people default is because some vendores enticed the customers into buying some house that they ended up not liking.

* __How likely is someone to default?__: This is a prediction question that is fully covered with logistic regression. However, as seen above, it is needed that we expand our logistic regression model since the off-the-shelf approach is not separating the data satisfactorily. 

\newpage

# Proposing models to address the questions

\newpage

# Laying out questions for feedback

Our main concerns are two:

* __Do we have the information necessary to address our research questions?__ Does the data that I presented in the first section appears suitable to address our questions? Is there anything that we might be asking that is ill-posed and would not be able to be seen from our data? Are we able to draw any type of causal inferences with out data? 

* __Are we posing a model that will allow us to derive the insights that we want?__ We are afraid that maybe the idea of modelling $\alpha_i$ and $I_i$ might be too restrictive as a family of models. Moreover, the inference for this model is also complicated since we do not have a conjugate family and thus it will requires to use gradient reparameterizations. We are also worried that we might be complicating our approach more than it is necessary. Should we better try to estimate a simpler model like a linear logisitic regression where we shared the parameters according differently (for example base $\beta_income$ we shared across zip codes but the $\beta_age$ we share across all individuals). 

* __Would incorporating historical data help us make any causal inferences?__ Our questions are centered about knowing why something happened. Furthermore, we have the same data for previous periods. Do you see any of those questions that might benefit from introducing the historical values of the same covariates? Also, should we introduce this historical dependencies in our model?
